{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np, sys,os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "import wfdb\n",
    "import tarfile\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import peak_widths\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "\n",
    "def load_challenge_data(filename):\n",
    "    x = loadmat(filename)\n",
    "    data = np.asarray(x['val'], dtype=np.float64)\n",
    "    new_file = filename.replace('.mat','.hea')\n",
    "    input_header_file = os.path.join(new_file)\n",
    "    with open(input_header_file,'r') as f:\n",
    "        header_data=f.readlines()\n",
    "    return data, header_data\n",
    "\n",
    "def clean_up_gender_data(gender):\n",
    "  gender = np.asarray(gender)\n",
    "  gender[np.where(gender == \"Male\")] = 0\n",
    "  gender[np.where(gender == \"male\")] = 0\n",
    "  gender[np.where(gender == \"M\")] = 0\n",
    "  gender[np.where(gender == \"Female\")] = 1\n",
    "  gender[np.where(gender == \"female\")] = 1\n",
    "  gender[np.where(gender == \"F\")] = 1\n",
    "  gender[np.where(gender == \"NaN\")] = 2\n",
    "  np.unique(gender)\n",
    "  gender = gender.astype(np.int)\n",
    "  return gender\n",
    "\n",
    "def clean_up_age_data(age):\n",
    "    age = np.asarray(age)\n",
    "    age[np.where(age == \"NaN\")] = -1\n",
    "    np.unique(age)\n",
    "    age = age.astype(np.int)\n",
    "    return age\n",
    "\n",
    "def import_gender_and_age(age, gender):\n",
    "    gender_binary = clean_up_gender_data(gender)\n",
    "    age_clean = clean_up_age_data(age)\n",
    "    print(\"gender data shape: {}\".format(gender_binary.shape[0]))\n",
    "    print(\"age data shape: {}\".format(age_clean.shape[0]))\n",
    "    return age_clean, gender_binary\n",
    "\n",
    "def import_key_data(path):\n",
    "    gender=[]\n",
    "    age=[]\n",
    "    labels=[]\n",
    "    ecg_filenames=[]\n",
    "    for subdir, dirs, files in sorted(os.walk(path)):\n",
    "        for filename in files:\n",
    "            filepath = subdir + os.sep + filename\n",
    "            if filepath.endswith(\".mat\"):\n",
    "                data, header_data = load_challenge_data(filepath)\n",
    "                labels.append(header_data[15][5:-1])\n",
    "                ecg_filenames.append(filepath)\n",
    "                gender.append(header_data[14][6:-1])\n",
    "                age.append(header_data[13][6:-1])\n",
    "    return gender, age, labels, ecg_filenames\n",
    "\n",
    "def get_signal_lengths(path, title):\n",
    "    signal_lenght=[]\n",
    "    for subdir, dirs, files in sorted(os.walk(path)):\n",
    "        for filename in files:\n",
    "            filepath = subdir + os.sep + filename\n",
    "            if filepath.endswith(\".mat\"):\n",
    "                data, header_data = load_challenge_data(filepath)\n",
    "                splitted = header_data[0].split()\n",
    "                signal_lenght.append(splitted[3])\n",
    "    signal_lenght_df = pd.DataFrame(signal_lenght)\n",
    "    signal_count=signal_lenght_df[0].value_counts()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(title,fontsize =36)\n",
    "    sns.barplot(signal_count[:10,].index, signal_count[:10,].values)\n",
    "      #plt.savefig(\"signallengde.png\")\n",
    "        \n",
    "def make_undefined_class(labels, df_unscored):\n",
    "    df_labels = pd.DataFrame(labels)\n",
    "    for i in range(len(df_unscored.iloc[0:,1])):\n",
    "        df_labels.replace(to_replace=str(df_unscored.iloc[i,1]), inplace=True ,value=\"undefined class\", regex=True)\n",
    "\n",
    "    '''\n",
    "    #equivalent classes\n",
    "    codes_to_replace=['713427006','284470004','427172004']\n",
    "    replace_with = ['59118001','63593006','17338001']\n",
    "\n",
    "    for i in range(len(codes_to_replace)):\n",
    "        df_labels.replace(to_replace=codes_to_replace[i], inplace=True ,value=replace_with[i], regex=True)\n",
    "    '''\n",
    "    return df_labels\n",
    "\n",
    "\n",
    "def onehot_encode(df_labels):\n",
    "    one_hot = MultiLabelBinarizer()\n",
    "    y=one_hot.fit_transform(df_labels[0].str.split(pat=','))\n",
    "    print(\"The classes we will look at are encoded as SNOMED CT codes:\")\n",
    "    print(one_hot.classes_)\n",
    "    y = np.delete(y, -1, axis=1)\n",
    "    print(\"classes: {}\".format(y.shape[1]))\n",
    "    return y, one_hot.classes_[0:-1]\n",
    "\n",
    "\n",
    "def plot_classes(classes, scored_classes,y):\n",
    "    for j in range(len(classes)):\n",
    "        for i in range(len(scored_classes.iloc[:,1])):\n",
    "            if (str(scored_classes.iloc[:,1][i]) == classes[j]):\n",
    "                classes[j] = scored_classes.iloc[:,0][i]\n",
    "    plt.figure(figsize=(30,20))\n",
    "    plt.bar(x=classes,height=y.sum(axis=0))\n",
    "    plt.title(\"Distribution of Diagnosis\", color = \"black\", fontsize = 30)\n",
    "    plt.tick_params(axis=\"both\", colors = \"black\")\n",
    "    plt.xlabel(\"Diagnosis\", color = \"black\")\n",
    "    plt.ylabel(\"Count\", color = \"black\")\n",
    "    plt.xticks(rotation=90, fontsize=20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.savefig(\"fordeling.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_labels_for_all_combinations(y):\n",
    "    y_all_combinations = LabelEncoder().fit_transform([''.join(str(l)) for l in y])\n",
    "    return y_all_combinations\n",
    "\n",
    "def split_data(labels, y_all_combo):\n",
    "    folds = list(StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(labels,y_all_combo))\n",
    "    print(\"Training split: {}\".format(len(folds[0][0])))\n",
    "    print(\"Validation split: {}\".format(len(folds[0][1])))\n",
    "    return folds\n",
    "\n",
    "def plot_all_folds(folds,y,onehot_enc):\n",
    "    X_axis_labels=onehot_enc\n",
    "    plt.figure(figsize=(20,100))\n",
    "    h=1\n",
    "    for i in range(len(folds)):\n",
    "        plt.subplot(10,2,h)\n",
    "        plt.subplots_adjust(hspace=1.0)\n",
    "        plt.bar(x= X_axis_labels, height=y[folds[i][0]].sum(axis=0))\n",
    "        plt.title(\"Distribution of Diagnosis - Training set - Fold {}\".format(i+1) ,fontsize=\"20\", color = \"black\")\n",
    "        plt.tick_params(axis=\"both\", colors = \"black\")\n",
    "        plt.xticks(rotation=90, fontsize=10)\n",
    "        plt.yticks(fontsize = 10)\n",
    "        #plt.xlabel(\"Diagnosis\", color = \"white\")\n",
    "        plt.ylabel(\"Count\", color = \"black\")\n",
    "        h=h+1\n",
    "        plt.subplot(10,2,h)\n",
    "        plt.subplots_adjust(hspace=1.0)\n",
    "        plt.bar(x= X_axis_labels, height=y[folds[i][1]].sum(axis=0))\n",
    "        plt.title(\"Distribution of Diagnosis - Validation set - Fold {}\".format(i+1) ,fontsize=\"20\", color = \"black\")\n",
    "        plt.tick_params(axis=\"both\", colors = \"black\")\n",
    "        #plt.xlabel(\"Diagnosis\", color = \"white\")\n",
    "        plt.ylabel(\"Count\", color = \"black\")\n",
    "        plt.xticks(rotation=90, fontsize=10)\n",
    "        plt.yticks(fontsize = 10)\n",
    "        h=h+1\n",
    "        \n",
    "def get_val_data(validation_filename):\n",
    "    ecg_val_timeseries=[]\n",
    "    for names in validation_filename:\n",
    "        data, header_data = pc.load_challenge_data(names)\n",
    "        data = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n",
    "        ecg_val_timeseries.append(data)\n",
    "    ecg_val_timeseries = np.asarray(ecg_val_timeseries)\n",
    "    return ecg_val_timeseries\n",
    "\n",
    "def generate_validation_data(ecg_filenames, y,test_order_array):\n",
    "    y_train_gridsearch=y[test_order_array]\n",
    "    ecg_filenames_train_gridsearch=ecg_filenames[test_order_array]\n",
    "\n",
    "    ecg_train_timeseries=[]\n",
    "    for names in ecg_filenames_train_gridsearch:\n",
    "        data, header_data = load_challenge_data(names)\n",
    "        data = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n",
    "        ecg_train_timeseries.append(data)\n",
    "    X_train_gridsearch = np.asarray(ecg_train_timeseries)\n",
    "\n",
    "    X_train_gridsearch = X_train_gridsearch.reshape(ecg_filenames_train_gridsearch.shape[0],5000,12)\n",
    "\n",
    "    return X_train_gridsearch, y_train_gridsearch\n",
    "\n",
    "def generate_validation_data_with_demo_data(ecg_filenames, y, gender, age, test_order_array):\n",
    "    y_train_gridsearch=y[test_order_array]\n",
    "    ecg_filenames_train_gridsearch=ecg_filenames[test_order_array]\n",
    "\n",
    "    ecg_train_timeseries=[]\n",
    "    for names in ecg_filenames_train_gridsearch:\n",
    "        data, header_data = load_challenge_data(names)\n",
    "        data = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n",
    "        ecg_train_timeseries.append(data)\n",
    "    X_val = np.asarray(ecg_train_timeseries)\n",
    "\n",
    "    X_val = X_val.reshape(ecg_filenames_train_gridsearch.shape[0],5000,12)\n",
    "    \n",
    "    age_val = age[test_order_array]\n",
    "    \n",
    "    gender_val = gender[test_order_array]\n",
    "    \n",
    "    demograpics_val_data = np.column_stack((age_val, gender_val))\n",
    "    X_combined_val = [X_val, demograpics_val_data]\n",
    "\n",
    "    return X_combined_val, y_train_gridsearch\n",
    "\n",
    "\n",
    "            \n",
    "def calculating_class_weights(y_true):\n",
    "    number_dim = np.shape(y_true)[1]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for i in range(number_dim):\n",
    "        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n",
    "    return weights\n",
    "\n",
    "\n",
    "def residual_network_1d():\n",
    "    n_feature_maps = 64\n",
    "    input_shape = (5000,12)\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    # BLOCK 1\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "    # BLOCK 3\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # no need to expand channels because they are equal\n",
    "    shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "    output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "    \n",
    "    # FINAL\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(27, activation='softmax')(gap_layer)\n",
    "\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def encoder_model():\n",
    "    input_layer = keras.layers.Input(shape=(5000, 12))\n",
    "\n",
    "\n",
    "     # conv block -1\n",
    "    conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(input_layer)\n",
    "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
    "    conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
    "    conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
    "    conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    # conv block -2\n",
    "    conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
    "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
    "    conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
    "    conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
    "    conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    # conv block -3\n",
    "    conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
    "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
    "    conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
    "    conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
    "    # split for attention\n",
    "    attention_data = keras.layers.Lambda(lambda x: x[:,:,:256])(conv3)\n",
    "    attention_softmax = keras.layers.Lambda(lambda x: x[:,:,256:])(conv3)\n",
    "    # attention mechanism\n",
    "    attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
    "    multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
    "    # last layer\n",
    "    dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(multiply_layer)\n",
    "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
    "    # output layer\n",
    "    flatten_layer = keras.layers.Flatten()(dense_layer)\n",
    "    output_layer = keras.layers.Dense(units=27,activation='sigmoid')(flatten_layer)\n",
    "\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "\n",
    "    return model\n",
    "\n",
    "def FCN():\n",
    "    inputlayer = keras.layers.Input(shape=(5000,12)) \n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8,input_shape=(5000,12), padding='same')(inputlayer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "\n",
    "    outputlayer = keras.layers.Dense(27, activation='sigmoid')(gap_layer)\n",
    "\n",
    "    model = keras.Model(inputs=inputlayer, outputs=outputlayer)\n",
    "  \n",
    "\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "\n",
    "    return model\n",
    "\n",
    "def residual_network_1d_demo():\n",
    "    n_feature_maps = 64\n",
    "    input_shape = (5000,12)\n",
    "    inputA = keras.layers.Input(input_shape)\n",
    "    inputB = keras.layers.Input(shape=(2,))\n",
    "\n",
    "    # BLOCK 1\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(inputA)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(inputA)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "    # BLOCK 3\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # no need to expand channels because they are equal\n",
    "    shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "    output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "    \n",
    "    # FINAL\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(27, activation='softmax')(gap_layer)\n",
    "\n",
    "    mod1 = keras.models.Model(inputs=inputA, outputs=output_layer)\n",
    "    \n",
    "\n",
    "    mod2 = keras.layers.Dense(50, activation=\"relu\")(inputB) \n",
    "    mod2 = keras.layers.Dense(2, activation=\"sigmoid\")(mod2) \n",
    "    mod2 = keras.models.Model(inputs=inputB, outputs=mod2)\n",
    "\n",
    "    combined = keras.layers.concatenate([mod1.output, mod2.output])\n",
    "\n",
    "    z = keras.layers.Dense(27, activation=\"sigmoid\")(combined)\n",
    "\n",
    "    model = keras.models.Model(inputs=[mod1.input, mod2.input], outputs=z)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def encoder_model_demo():\n",
    "    inputA = keras.layers.Input(shape=(5000, 12))\n",
    "    inputB = keras.layers.Input(shape=(2,))\n",
    "    # conv block -1\n",
    "    conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(inputA)\n",
    "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
    "    conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
    "    conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
    "    conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    # conv block -2\n",
    "    conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
    "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
    "    conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
    "    conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
    "    conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    # conv block -3\n",
    "    conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
    "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
    "    conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
    "    conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
    "    # split for attention\n",
    "    attention_data = keras.layers.Lambda(lambda x: x[:,:,:256])(conv3)\n",
    "    attention_softmax = keras.layers.Lambda(lambda x: x[:,:,256:])(conv3)\n",
    "    # attention mechanism\n",
    "    attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
    "    multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
    "    # last layer\n",
    "    dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(multiply_layer)\n",
    "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
    "    # output layer\n",
    "\n",
    "\n",
    "    output_layer = keras.layers.Flatten()(dense_layer)  \n",
    "\n",
    "    mod1 = keras.Model(inputs=inputA, outputs=output_layer)\n",
    "\n",
    "    mod2 = keras.layers.Dense(50, activation=\"relu\")(inputB) \n",
    "    mod2 = keras.layers.Dense(2, activation=\"sigmoid\")(mod2) \n",
    "    mod2 = keras.models.Model(inputs=inputB, outputs=mod2)\n",
    "\n",
    "    combined = keras.layers.concatenate([mod1.output, mod2.output])\n",
    "\n",
    "    z = keras.layers.Dense(27, activation=\"sigmoid\")(combined)\n",
    "\n",
    "    model = keras.models.Model(inputs=[mod1.input, mod2.input], outputs=z)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "\n",
    "    return model\n",
    "\n",
    "def FCN_demo():\n",
    "\n",
    "    inputA = keras.layers.Input(shape=(5000,12))\n",
    "    inputB = keras.layers.Input(shape=(2,))\n",
    "  \n",
    "  \n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8,input_shape=(5000,12), padding='same')(inputA)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    model1 = keras.Model(inputs=inputA, outputs=gap_layer)\n",
    "\n",
    "\n",
    "\n",
    "    mod3 = keras.layers.Dense(50, activation=\"relu\")(inputB) \n",
    "    mod3 = keras.layers.Dense(2, activation=\"sigmoid\")(mod3) \n",
    "    model3 = keras.Model(inputs=inputB, outputs=mod3)\n",
    "\n",
    "    combined = keras.layers.concatenate([model1.output, model3.output])\n",
    "    final_layer = keras.layers.Dense(27, activation=\"sigmoid\")(combined)\n",
    "    model = keras.models.Model(inputs=[inputA,inputB], outputs=final_layer)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "    return model\n",
    "\n",
    "def FCN_Encoder():\n",
    "\n",
    "    inputA = tf.keras.layers.Input(shape=(5000,12))\n",
    "\n",
    "  \n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8,input_shape=(5000,12), padding='same')(inputA)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    model1 = keras.Model(inputs=inputA, outputs=gap_layer)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(inputA)\n",
    "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
    "    conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
    "    conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
    "    conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    # conv block -2\n",
    "    conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
    "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
    "    conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
    "    conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
    "    conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    # conv block -3\n",
    "    conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
    "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
    "    conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
    "    conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
    "    # split for attention\n",
    "    attention_data = keras.layers.Lambda(lambda x: x[:,:,:256])(conv3)\n",
    "    attention_softmax = keras.layers.Lambda(lambda x: x[:,:,256:])(conv3)\n",
    "    # attention mechanism\n",
    "    attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
    "    multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
    "    # last layer\n",
    "    dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(multiply_layer)\n",
    "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
    "    # output layer\n",
    "    flatten_layer = keras.layers.Flatten()(dense_layer)\n",
    "    model2 = keras.Model(inputs=inputA, outputs=flatten_layer)\n",
    "\n",
    "    combined = keras.layers.concatenate([model1.output, model2.output])\n",
    "    final_layer = keras.layers.Dense(27, activation=\"sigmoid\")(combined)\n",
    "    model = keras.models.Model(inputs=inputA, outputs=final_layer)\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def FCN_Encoder_demo():\n",
    "\n",
    "    inputA = keras.layers.Input(shape=(5000,12))\n",
    "    inputB = keras.layers.Input(shape=(2,))\n",
    "\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8,input_shape=(5000,12), padding='same')(inputA)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    model1 = keras.Model(inputs=inputA, outputs=gap_layer)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=256,kernel_size=10,strides=1,padding='same')(inputA)\n",
    "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
    "    conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
    "    conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
    "    conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    # conv block -2\n",
    "    conv2 = keras.layers.Conv1D(filters=512,kernel_size=22,strides=1,padding='same')(conv1)\n",
    "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
    "    conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
    "    conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
    "    conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    # conv block -3\n",
    "    conv3 = keras.layers.Conv1D(filters=1024,kernel_size=42,strides=1,padding='same')(conv2)\n",
    "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
    "    conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
    "    conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
    "    # split for attention\n",
    "    attention_data = keras.layers.Lambda(lambda x: x[:,:,:512])(conv3)\n",
    "    attention_softmax = keras.layers.Lambda(lambda x: x[:,:,512:])(conv3)\n",
    "    # attention mechanism\n",
    "    attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
    "    multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
    "    # last layer\n",
    "    dense_layer = keras.layers.Dense(units=512,activation='sigmoid')(multiply_layer)\n",
    "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
    "    # output layer\n",
    "    flatten_layer = keras.layers.Flatten()(dense_layer)\n",
    "    model2 = keras.Model(inputs=inputA, outputs=flatten_layer)\n",
    "\n",
    "\n",
    "    mod3 = keras.layers.Dense(50, activation=\"relu\")(inputB) # 2 -> 100\n",
    "    mod3 = keras.layers.Dense(2, activation=\"sigmoid\")(mod3) # Added this layer\n",
    "    model3 = keras.Model(inputs=inputB, outputs=mod3)\n",
    "\n",
    "    combined = keras.layers.concatenate([model1.output, model2.output, model3.output])\n",
    "    final_layer = keras.layers.Dense(27, activation=\"sigmoid\")(combined)\n",
    "    model = keras.models.Model(inputs=[inputA,inputB], outputs=final_layer)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "            name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                        tf.keras.metrics.AUC(\n",
    "            num_thresholds=200,\n",
    "            curve=\"ROC\",\n",
    "            summation_method=\"interpolation\",\n",
    "            name=\"AUC\",\n",
    "            dtype=None,\n",
    "            thresholds=None,\n",
    "            multi_label=True,\n",
    "            label_weights=None,\n",
    "        )])\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_challenge_metric_for_opt(labels, outputs):\n",
    "    classes=['10370003','111975006','164889003','164890007','164909002','164917005','164934002','164947007','17338001',\n",
    " '251146004','270492004','284470004','39732003','426177001','426627000','426783006','427084000','427172004','427393009','445118002','47665007','59118001',\n",
    " '59931005','63593006','698252002','713426002','713427006']\n",
    "\n",
    "    '''\n",
    "    24 classes\n",
    "    ['10370003', '111975006', '164889003', '164890007', '164909002', '164917005',\n",
    "      '164934002', '164947007', '17338001', '251146004', '270492004', '39732003',\n",
    "      '426177001', '426627000', '426783006' ,'427084000' ,'427393009', '445118002',\n",
    "      '47665007' ,'59118001', '59931005', '63593006', '698252002', '713426002']\n",
    "\n",
    "    \n",
    "      \n",
    "    '''\n",
    "\n",
    "    normal_class = '426783006'\n",
    "    weights = np.array([[1.    , 0.425 , 0.375 , 0.375 , 0.4   , 0.275 , 0.375 , 0.425 ,\n",
    "        0.5   , 0.45  , 0.425 , 0.4625, 0.475 , 0.425 , 0.425 , 0.375 ,\n",
    "        0.5   , 0.5   , 0.425 , 0.475 , 0.475 , 0.475 , 0.375 , 0.4625,\n",
    "        0.475 , 0.425 , 0.475 ],\n",
    "       [0.425 , 1.    , 0.45  , 0.45  , 0.475 , 0.35  , 0.45  , 0.35  ,\n",
    "        0.425 , 0.475 , 0.35  , 0.3875, 0.4   , 0.35  , 0.35  , 0.3   ,\n",
    "        0.425 , 0.425 , 0.35  , 0.4   , 0.4   , 0.45  , 0.45  , 0.3875,\n",
    "        0.4   , 0.35  , 0.45  ],\n",
    "       [0.375 , 0.45  , 1.    , 0.5   , 0.475 , 0.4   , 0.5   , 0.3   ,\n",
    "        0.375 , 0.425 , 0.3   , 0.3375, 0.35  , 0.3   , 0.3   , 0.25  ,\n",
    "        0.375 , 0.375 , 0.3   , 0.35  , 0.35  , 0.4   , 0.5   , 0.3375,\n",
    "        0.35  , 0.3   , 0.4   ],\n",
    "       [0.375 , 0.45  , 0.5   , 1.    , 0.475 , 0.4   , 0.5   , 0.3   ,\n",
    "        0.375 , 0.425 , 0.3   , 0.3375, 0.35  , 0.3   , 0.3   , 0.25  ,\n",
    "        0.375 , 0.375 , 0.3   , 0.35  , 0.35  , 0.4   , 0.5   , 0.3375,\n",
    "        0.35  , 0.3   , 0.4   ],\n",
    "       [0.4   , 0.475 , 0.475 , 0.475 , 1.    , 0.375 , 0.475 , 0.325 ,\n",
    "        0.4   , 0.45  , 0.325 , 0.3625, 0.375 , 0.325 , 0.325 , 0.275 ,\n",
    "        0.4   , 0.4   , 0.325 , 0.375 , 0.375 , 0.425 , 0.475 , 0.3625,\n",
    "        0.375 , 0.325 , 0.425 ],\n",
    "       [0.275 , 0.35  , 0.4   , 0.4   , 0.375 , 1.    , 0.4   , 0.2   ,\n",
    "        0.275 , 0.325 , 0.2   , 0.2375, 0.25  , 0.2   , 0.2   , 0.15  ,\n",
    "        0.275 , 0.275 , 0.2   , 0.25  , 0.25  , 0.3   , 0.4   , 0.2375,\n",
    "        0.25  , 0.2   , 0.3   ],\n",
    "       [0.375 , 0.45  , 0.5   , 0.5   , 0.475 , 0.4   , 1.    , 0.3   ,\n",
    "        0.375 , 0.425 , 0.3   , 0.3375, 0.35  , 0.3   , 0.3   , 0.25  ,\n",
    "        0.375 , 0.375 , 0.3   , 0.35  , 0.35  , 0.4   , 0.5   , 0.3375,\n",
    "        0.35  , 0.3   , 0.4   ],\n",
    "       [0.425 , 0.35  , 0.3   , 0.3   , 0.325 , 0.2   , 0.3   , 1.    ,\n",
    "        0.425 , 0.375 , 0.5   , 0.4625, 0.45  , 0.5   , 0.5   , 0.45  ,\n",
    "        0.425 , 0.425 , 0.5   , 0.45  , 0.45  , 0.4   , 0.3   , 0.4625,\n",
    "        0.45  , 0.5   , 0.4   ],\n",
    "       [0.5   , 0.425 , 0.375 , 0.375 , 0.4   , 0.275 , 0.375 , 0.425 ,\n",
    "        1.    , 0.45  , 0.425 , 0.4625, 0.475 , 0.425 , 0.425 , 0.375 ,\n",
    "        0.5   , 1.    , 0.425 , 0.475 , 0.475 , 0.475 , 0.375 , 0.4625,\n",
    "        0.475 , 0.425 , 0.475 ],\n",
    "       [0.45  , 0.475 , 0.425 , 0.425 , 0.45  , 0.325 , 0.425 , 0.375 ,\n",
    "        0.45  , 1.    , 0.375 , 0.4125, 0.425 , 0.375 , 0.375 , 0.325 ,\n",
    "        0.45  , 0.45  , 0.375 , 0.425 , 0.425 , 0.475 , 0.425 , 0.4125,\n",
    "        0.425 , 0.375 , 0.475 ],\n",
    "       [0.425 , 0.35  , 0.3   , 0.3   , 0.325 , 0.2   , 0.3   , 0.5   ,\n",
    "        0.425 , 0.375 , 1.    , 0.4625, 0.45  , 0.5   , 0.5   , 0.45  ,\n",
    "        0.425 , 0.425 , 0.5   , 0.45  , 0.45  , 0.4   , 0.3   , 0.4625,\n",
    "        0.45  , 0.5   , 0.4   ],\n",
    "       [0.4625, 0.3875, 0.3375, 0.3375, 0.3625, 0.2375, 0.3375, 0.4625,\n",
    "        0.4625, 0.4125, 0.4625, 1.    , 0.4875, 0.4625, 0.4625, 0.4125,\n",
    "        0.4625, 0.4625, 0.4625, 0.4875, 0.4875, 0.4375, 0.3375, 1.    ,\n",
    "        0.4875, 0.4625, 0.4375],\n",
    "       [0.475 , 0.4   , 0.35  , 0.35  , 0.375 , 0.25  , 0.35  , 0.45  ,\n",
    "        0.475 , 0.425 , 0.45  , 0.4875, 1.    , 0.45  , 0.45  , 0.4   ,\n",
    "        0.475 , 0.475 , 0.45  , 0.5   , 0.5   , 0.45  , 0.35  , 0.4875,\n",
    "        0.5   , 0.45  , 0.45  ],\n",
    "       [0.425 , 0.35  , 0.3   , 0.3   , 0.325 , 0.2   , 0.3   , 0.5   ,\n",
    "        0.425 , 0.375 , 0.5   , 0.4625, 0.45  , 1.    , 0.5   , 0.45  ,\n",
    "        0.425 , 0.425 , 0.5   , 0.45  , 0.45  , 0.4   , 0.3   , 0.4625,\n",
    "        0.45  , 0.5   , 0.4   ],\n",
    "       [0.425 , 0.35  , 0.3   , 0.3   , 0.325 , 0.2   , 0.3   , 0.5   ,\n",
    "        0.425 , 0.375 , 0.5   , 0.4625, 0.45  , 0.5   , 1.    , 0.45  ,\n",
    "        0.425 , 0.425 , 0.5   , 0.45  , 0.45  , 0.4   , 0.3   , 0.4625,\n",
    "        0.45  , 0.5   , 0.4   ],\n",
    "       [0.375 , 0.3   , 0.25  , 0.25  , 0.275 , 0.15  , 0.25  , 0.45  ,\n",
    "        0.375 , 0.325 , 0.45  , 0.4125, 0.4   , 0.45  , 0.45  , 1.    ,\n",
    "        0.375 , 0.375 , 0.45  , 0.4   , 0.4   , 0.35  , 0.25  , 0.4125,\n",
    "        0.4   , 0.45  , 0.35  ],\n",
    "       [0.5   , 0.425 , 0.375 , 0.375 , 0.4   , 0.275 , 0.375 , 0.425 ,\n",
    "        0.5   , 0.45  , 0.425 , 0.4625, 0.475 , 0.425 , 0.425 , 0.375 ,\n",
    "        1.    , 0.5   , 0.425 , 0.475 , 0.475 , 0.475 , 0.375 , 0.4625,\n",
    "        0.475 , 0.425 , 0.475 ],\n",
    "       [0.5   , 0.425 , 0.375 , 0.375 , 0.4   , 0.275 , 0.375 , 0.425 ,\n",
    "        1.    , 0.45  , 0.425 , 0.4625, 0.475 , 0.425 , 0.425 , 0.375 ,\n",
    "        0.5   , 1.    , 0.425 , 0.475 , 0.475 , 0.475 , 0.375 , 0.4625,\n",
    "        0.475 , 0.425 , 0.475 ],\n",
    "       [0.425 , 0.35  , 0.3   , 0.3   , 0.325 , 0.2   , 0.3   , 0.5   ,\n",
    "        0.425 , 0.375 , 0.5   , 0.4625, 0.45  , 0.5   , 0.5   , 0.45  ,\n",
    "        0.425 , 0.425 , 1.    , 0.45  , 0.45  , 0.4   , 0.3   , 0.4625,\n",
    "        0.45  , 0.5   , 0.4   ],\n",
    "       [0.475 , 0.4   , 0.35  , 0.35  , 0.375 , 0.25  , 0.35  , 0.45  ,\n",
    "        0.475 , 0.425 , 0.45  , 0.4875, 0.5   , 0.45  , 0.45  , 0.4   ,\n",
    "        0.475 , 0.475 , 0.45  , 1.    , 0.5   , 0.45  , 0.35  , 0.4875,\n",
    "        0.5   , 0.45  , 0.45  ],\n",
    "       [0.475 , 0.4   , 0.35  , 0.35  , 0.375 , 0.25  , 0.35  , 0.45  ,\n",
    "        0.475 , 0.425 , 0.45  , 0.4875, 0.5   , 0.45  , 0.45  , 0.4   ,\n",
    "        0.475 , 0.475 , 0.45  , 0.5   , 1.    , 0.45  , 0.35  , 0.4875,\n",
    "        0.5   , 0.45  , 0.45  ],\n",
    "       [0.475 , 0.45  , 0.4   , 0.4   , 0.425 , 0.3   , 0.4   , 0.4   ,\n",
    "        0.475 , 0.475 , 0.4   , 0.4375, 0.45  , 0.4   , 0.4   , 0.35  ,\n",
    "        0.475 , 0.475 , 0.4   , 0.45  , 0.45  , 1.    , 0.4   , 0.4375,\n",
    "        0.45  , 0.4   , 1.    ],\n",
    "       [0.375 , 0.45  , 0.5   , 0.5   , 0.475 , 0.4   , 0.5   , 0.3   ,\n",
    "        0.375 , 0.425 , 0.3   , 0.3375, 0.35  , 0.3   , 0.3   , 0.25  ,\n",
    "        0.375 , 0.375 , 0.3   , 0.35  , 0.35  , 0.4   , 1.    , 0.3375,\n",
    "        0.35  , 0.3   , 0.4   ],\n",
    "       [0.4625, 0.3875, 0.3375, 0.3375, 0.3625, 0.2375, 0.3375, 0.4625,\n",
    "        0.4625, 0.4125, 0.4625, 1.    , 0.4875, 0.4625, 0.4625, 0.4125,\n",
    "        0.4625, 0.4625, 0.4625, 0.4875, 0.4875, 0.4375, 0.3375, 1.    ,\n",
    "        0.4875, 0.4625, 0.4375],\n",
    "       [0.475 , 0.4   , 0.35  , 0.35  , 0.375 , 0.25  , 0.35  , 0.45  ,\n",
    "        0.475 , 0.425 , 0.45  , 0.4875, 0.5   , 0.45  , 0.45  , 0.4   ,\n",
    "        0.475 , 0.475 , 0.45  , 0.5   , 0.5   , 0.45  , 0.35  , 0.4875,\n",
    "        1.    , 0.45  , 0.45  ],\n",
    "       [0.425 , 0.35  , 0.3   , 0.3   , 0.325 , 0.2   , 0.3   , 0.5   ,\n",
    "        0.425 , 0.375 , 0.5   , 0.4625, 0.45  , 0.5   , 0.5   , 0.45  ,\n",
    "        0.425 , 0.425 , 0.5   , 0.45  , 0.45  , 0.4   , 0.3   , 0.4625,\n",
    "        0.45  , 1.    , 0.4   ],\n",
    "       [0.475 , 0.45  , 0.4   , 0.4   , 0.425 , 0.3   , 0.4   , 0.4   ,\n",
    "        0.475 , 0.475 , 0.4   , 0.4375, 0.45  , 0.4   , 0.4   , 0.35  ,\n",
    "        0.475 , 0.475 , 0.4   , 0.45  , 0.45  , 1.    , 0.4   , 0.4375,\n",
    "        0.45  , 0.4   , 1.    ]])\n",
    "    \n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    normal_index = classes.index(normal_class)\n",
    "\n",
    "    # Compute the observed score.\n",
    "    A = compute_modified_confusion_matrix(labels, outputs)\n",
    "    observed_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the correct label(s).\n",
    "    correct_outputs = labels\n",
    "    A = compute_modified_confusion_matrix(labels, correct_outputs)\n",
    "    correct_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the normal class.\n",
    "    inactive_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n",
    "    inactive_outputs[:, normal_index] = 1\n",
    "    A = compute_modified_confusion_matrix(labels, inactive_outputs)\n",
    "    inactive_score = np.nansum(weights * A)\n",
    "\n",
    "    if correct_score != inactive_score:\n",
    "        normalized_score = float(observed_score - inactive_score) / float(correct_score - inactive_score)\n",
    "    else:\n",
    "        normalized_score = float('nan')\n",
    "\n",
    "    return normalized_score\n",
    "\n",
    "def compute_modified_confusion_matrix(labels, outputs):\n",
    "    # Compute a binary multi-class, multi-label confusion matrix, where the rows\n",
    "    # are the labels and the columns are the outputs.\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    A = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    # Iterate over all of the recordings.\n",
    "    for i in range(num_recordings):\n",
    "        # Calculate the number of positive labels and/or outputs.\n",
    "        normalization = float(max(np.sum(np.any((labels[i, :], outputs[i, :]), axis=0)), 1))\n",
    "        # Iterate over all of the classes.\n",
    "        for j in range(num_classes):\n",
    "            # Assign full and/or partial credit for each positive class.\n",
    "            if labels[i, j]:\n",
    "                for k in range(num_classes):\n",
    "                    if outputs[i, k]:\n",
    "                        A[j, k] += 1.0/normalization\n",
    "\n",
    "    return A\n",
    "\n",
    "def iterate_threshold(y_pred, ecg_filenames, y ,val_fold ):\n",
    "    init_thresholds = np.arange(0,1,0.05)\n",
    "    \n",
    "    all_scores = []\n",
    "    for i in init_thresholds:\n",
    "        pred_output = y_pred > i\n",
    "        pred_output = pred_output * 1\n",
    "        score = compute_challenge_metric_for_opt(generate_validation_data(ecg_filenames,y,val_fold)[1],pred_output)\n",
    "        print(score)\n",
    "        all_scores.append(score)\n",
    "    all_scores = np.asarray(all_scores)\n",
    "    \n",
    "    return all_scores\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_normalized_conf_matrix(y_pred, ecg_filenames, y, val_fold, threshold, snomedclasses, snomedabbr):\n",
    "    conf_m = compute_modified_confusion_matrix(generate_validation_data(ecg_filenames,y,val_fold)[1], (y_pred>threshold)*1)\n",
    "    conf_m = np.nan_to_num(conf_m)\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #conf_m_scaled = min_max_scaler.fit_transform(conf_m)\n",
    "    normalizer = preprocessing.Normalizer(norm=\"l1\")\n",
    "    conf_m_scaled = normalizer.fit_transform(conf_m)\n",
    "    df_norm_col = pd.DataFrame(conf_m_scaled)\n",
    "    df_norm_col.columns = snomedabbr\n",
    "    df_norm_col.index = snomedabbr\n",
    "    df_norm_col.index.name = 'Actual'\n",
    "    df_norm_col.columns.name = 'Predicted'\n",
    "    #df_norm_col=(df_cm-df_cm.mean())/df_cm.std()\n",
    "    plt.figure(figsize = (12,10))\n",
    "    sns.set(font_scale=1.4)#for label size\n",
    "    sns.heatmap(df_norm_col, cmap=\"rocket_r\", annot=True,cbar=False, annot_kws={\"size\": 10},fmt=\".2f\")# \n",
    "#############################\n",
    "#Adding rule-based algorithms\n",
    "#############################\n",
    "\n",
    "def DetectRWithPanTompkins (signal, signal_freq):\n",
    "    '''signal=ECG signal (type=np.array), signal_freq=sample frequenzy'''\n",
    "    lowcut = 5.0\n",
    "    highcut = 15.0\n",
    "    filter_order = 2\n",
    "    \n",
    "    nyquist_freq = 0.5 * signal_freq\n",
    "    low = lowcut / nyquist_freq\n",
    "    high = highcut / nyquist_freq\n",
    "    \n",
    "    b, a = butter(filter_order, [low, high], btype=\"band\")\n",
    "    y = lfilter(b, a, signal)\n",
    "    \n",
    "    diff_y=np.ediff1d(y)\n",
    "    squared_diff_y=diff_y**2\n",
    "    \n",
    "    integrated_squared_diff_y =np.convolve(squared_diff_y,np.ones(5))\n",
    "    \n",
    "    normalized = (integrated_squared_diff_y-min(integrated_squared_diff_y))/(max(integrated_squared_diff_y)-min(integrated_squared_diff_y))\n",
    "\n",
    "    peaks, metadata = find_peaks(normalized, \n",
    "                             distance=signal_freq/5 , \n",
    "                             #height=500,\n",
    "                             height=0.5,\n",
    "                             width=0.5\n",
    "                            )\n",
    "\n",
    "    return peaks\n",
    "\n",
    "def heartrate(r_time, sampfreq):\n",
    "    \n",
    "    #qrs = xqrs.qrs_inds from annotateR()\n",
    "    #sampfreq = sample frequency - can be found with y['fs'] (from getDataFromPhysionet())\n",
    "    \n",
    "    HeartRate = []\n",
    "    TimeBetweenBeat= []\n",
    "    for index, item in enumerate(r_time,-1):\n",
    "        HeartRate.append(60/((r_time[index+1]-r_time[index])/sampfreq))\n",
    "        TimeBetweenBeat.append((r_time[index+1]-r_time[index])/sampfreq)\n",
    "    del HeartRate[0]\n",
    "    avgHr = sum(HeartRate)/len(HeartRate)\n",
    "    TimeBetweenBeat= np.asarray(TimeBetweenBeat)\n",
    "    TimeBetweenBeat=TimeBetweenBeat * 1000 # sec to ms\n",
    "    TimeBetweenBeat = TimeBetweenBeat[1:] # remove first element\n",
    "    return TimeBetweenBeat, avgHr\n",
    "\n",
    "def R_correction(signal, peaks):\n",
    "    '''signal = ECG signal, peaks = uncorrected R peaks'''\n",
    "    peaks_corrected, metadata = find_peaks(signal, distance=min(np.diff(peaks)))            \n",
    "    return peaks_corrected\n",
    "\n",
    "def rule_based_predictions(ecgfilenames, val_data, dnn_prediction):\n",
    "    for i in range(len(val_data)):\n",
    "        data , header_data = load_challenge_data(ecgfilenames[val_data[i]])\n",
    "\n",
    "        avg_hr = 0\n",
    "        peaks = 0\n",
    "        rmssd = 0\n",
    "        qrs_voltage = 0\n",
    "        try:\n",
    "            peaks = DetectRWithPanTompkins(data[1],int(header_data[0].split()[2]))\n",
    "\n",
    "            try:\n",
    "                peaks = R_correction(data[1], peaks)\n",
    "            except:\n",
    "                print(\"Did not manage to do R_correction\")\n",
    "\n",
    "        except:\n",
    "            print(\"Did not manage to find any peaks using Pan Tomkins\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            rr_interval, avg_hr = heartrate(peaks,int(header_data[0].split()[2]))\n",
    "            try:\n",
    "                rmssd = np.mean(np.square(np.diff(rr_interval)))\n",
    "            except:\n",
    "                print(\"did not manage to comp rmssd\")\n",
    "        except:\n",
    "            print(\"not able to calculate heart rate\")\n",
    "            rr_interval = 0\n",
    "            avg_hr = 0\n",
    "\n",
    "        try:\n",
    "            qrs_voltage = np.mean(data[1][peaks])\n",
    "        except:\n",
    "            print(\"Could not calculate mean QRS peak voltage\")\n",
    "\n",
    "        if avg_hr != 0:     # bare gjr disse endringene dersom vi klarer  beregne puls\n",
    "            if 60 < avg_hr < 100:\n",
    "                dnn_prediction[i][16] = 0\n",
    "                dnn_prediction[i][14] = 0\n",
    "                dnn_prediction[i][13] = 0\n",
    "            elif avg_hr < 60 & dnn_prediction[i][15] == 1:\n",
    "                dnn_prediction[i][13] = 1\n",
    "            elif avg_hr < 60 & dnn_prediction[i][15] == 0:\n",
    "                dnn_prediction[i][14] = 1\n",
    "            elif avg_hr > 100:\n",
    "                dnn_prediction[i][16] = 1\n",
    "\n",
    "        if qrs_voltage != 0:\n",
    "            if qrs_voltage < 500:\n",
    "                dnn_prediction[i][9] = 1\n",
    "                dnn_prediction[i][15] = 0\n",
    "            else:\n",
    "                dnn_prediction[i][9] = 0\n",
    "        else:\n",
    "            dnn_prediction[i][9] = 0\n",
    "\n",
    "        if rmssd != 0:\n",
    "            if rmssd < 15:\n",
    "                dnn_prediction[i][0] = 1\n",
    "                dnn_prediction[i][16] = 0\n",
    "                dnn_prediction[i][15] = 0\n",
    "                dnn_prediction[i][14] = 0\n",
    "                dnn_prediction[i][13] = 0\n",
    "            elif 2000 < rmssd < 5000:\n",
    "                dnn_prediction[i][18] = 1\n",
    "            elif 15000 < rmssd < 50000:\n",
    "                dnn_prediction[i][2] = 1\n",
    "            else:\n",
    "                dnn_prediction[i][15] = 1\n",
    "    return dnn_prediction\n",
    "\n",
    "def plot_normalized_conf_matrix_rule(y_true,val_data,rb_pred,snomedclasses):\n",
    "    df_cm = pd.DataFrame(compute_modified_confusion_matrix(y_true[val_data],rb_pred), columns=snomedclasses, index = snomedclasses)\n",
    "    df_cm = df_cm.fillna(0)\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    df_norm_col=(df_cm-df_cm.mean())/df_cm.std()\n",
    "    plt.figure(figsize = (36,14))\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(df_norm_col, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16},fmt=\".2f\",cbar=False)# font size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
